# Solving Gambler's Problem

I have implemented Action-Value based DP and State-Value based DP.


In State-Value based DP, 
I implemented DP with two different ways: Bellman Optimality Equation and Bellman Expectation Equation.
Action-Value based DP is implementeted with Q_action value.




There is no big difference in three methods.



action value graph
```bash
python action_value.py
```
state value graph
```bash
python state_value.py
```